{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 Die TensorFlow-Autoren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/advanced_autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Auf TensorFlow.org ansehen</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">In Google Colab ausführen</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Quelle auf GitHub anzeigen</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/advanced_autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Notizbuch herunterladen</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "Die [automatische Differenzierungshilfe](autodiff.ipynb) beinhaltet alles, was zur Berechnung von Steigungen benötigt wird. Dieser Leitfaden konzentriert sich auf tiefere, weniger verbreitete Funktionen der `tf.GradientTape` API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Aufstellen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl_FywGZpIeFf"
      },
      "source": [
        "# 2222"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl_iHAob3plC6"
      },
      "outputs": [

      ],
      "source": [
        "lood777.444"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uGRJJRi8TCkJ"
      },
      "source": [
        "## Steuern der Verlaufsaufzeichnung\n",
        "\n",
        "In der [Anleitung zur automatischen Differenzierung haben](autodiff.ipynb) Sie gesehen, wie Sie steuern, welche Variablen und Tensoren beim Erstellen der Gradientenberechnung vom Band überwacht werden.\n",
        "\n",
        "Das Band verfügt auch über Methoden, um die Aufnahme zu manipulieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gB_i0VnhQKt2"
      },
      "source": [
        "Wenn Sie die Aufnahme von Farbverläufen stoppen möchten, können Sie `GradientTape.stop_recording()` um die Aufnahme vorübergehend zu unterbrechen.\n",
        "\n",
        "Dies kann nützlich sein, um den Overhead zu reduzieren, wenn Sie keine komplizierten Operationen in der Mitte Ihres Modells unterscheiden möchten. Dies könnte die Berechnung einer Metrik oder eines Zwischenergebnisses beinhalten:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "mhFSYf7uQWxR"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  x_sq = x * x\n",
        "  with t.stop_recording():\n",
        "    y_sq = y * y\n",
        "  z = x_sq + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DEHbEZ1h4p8A"
      },
      "source": [
        "Wenn Sie ganz von vorne beginnen möchten, verwenden Sie `reset()` . Das einfache Beenden des Gradientenbandblocks und das erneute Starten ist normalerweise einfacher zu lesen, aber Sie können `reset` wenn das Verlassen des Bandblocks schwierig oder unmöglich ist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "lsMHsmrh4pqM"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "reset = True\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y * y\n",
        "  if reset:\n",
        "    # Throw out all the tape recorded so far\n",
        "    t.reset()\n",
        "  z = x * x + y_sq\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6zS7cLmS6zMf"
      },
      "source": [
        "## Steigung stoppen\n",
        "\n",
        "Im Gegensatz zu den oben genannten globalen Tape-Steuerelementen ist die `tf.stop_gradient` viel präziser. Es kann verwendet werden, um zu verhindern, dass Gradienten entlang eines bestimmten Pfads fließen, ohne dass Sie auf das Band selbst zugreifen müssen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "30qnZMe48BkB"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  y_sq = y**2\n",
        "  z = x**2 + tf.stop_gradient(y_sq)\n",
        "\n",
        "grad = t.gradient(z, {'x': x, 'y': y})\n",
        "\n",
        "print('dz/dx:', grad['x'])  # 2*x => 4\n",
        "print('dz/dy:', grad['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mbb-9lnGVngH"
      },
      "source": [
        "## Benutzerdefinierte Farbverläufe\n",
        "\n",
        "In einigen Fällen möchten Sie möglicherweise genau steuern, wie Farbverläufe berechnet werden, anstatt den Standardwert zu verwenden. Zu diesen Situationen gehören:\n",
        "\n",
        "- Es gibt keinen definierten Farbverlauf für eine neue Op, die Sie schreiben.\n",
        "- Die Standardberechnungen sind numerisch instabil.\n",
        "- Sie möchten eine teure Berechnung aus dem Vorwärtsdurchlauf zwischenspeichern.\n",
        "- Sie möchten einen Wert ändern (zum Beispiel mit: `tf.clip_by_value` , `tf.math.round` ), ohne den Farbverlauf zu ändern.\n",
        "\n",
        "Um eine neue Op zu schreiben, können Sie `tf.RegisterGradient` , um Ihre eigene einzurichten. Siehe diese Seite für Details. (Beachten Sie, dass die Verlaufsregistrierung global ist, ändern Sie sie also mit Vorsicht.)\n",
        "\n",
        "Für die letzten drei Fälle können Sie `tf.custom_gradient` .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oHr31kc_irF_"
      },
      "source": [
        "Hier ist ein Beispiel, das `tf.clip_by_norm` auf den Zwischengradienten anwendet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Mjj01w4NYtwd"
      },
      "outputs": [

      ],
      "source": [
        "# Establish an identity operation, but clip during the gradient pass\n",
        "@tf.custom_gradient\n",
        "def clip_gradients(y):\n",
        "  def backward(dy):\n",
        "    return tf.clip_by_norm(dy, 0.5)\n",
        "  return y, backward\n",
        "\n",
        "v = tf.Variable(2.0)\n",
        "with tf.GradientTape() as t:\n",
        "  output = clip_gradients(v * v)\n",
        "print(t.gradient(output, v))  # calls \"backward\", which clips 4 to 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n4t7S0scYrD3"
      },
      "source": [
        "Weitere `tf.custom_gradient` Dekorator tf.custom_gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8aENEt6Veryb"
      },
      "source": [
        "## Mehrere Bänder\n",
        "\n",
        "Mehrere Bänder interagieren nahtlos. Hier zum Beispiel beobachtet jedes Band einen anderen Satz von Tensoren:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "BJ0HdMvte0VZ"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.constant(0.0)\n",
        "x1 = tf.constant(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
        "  tape0.watch(x0)\n",
        "  tape1.watch(x1)\n",
        "\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.sigmoid(x1)\n",
        "\n",
        "  y = y0 + y1\n",
        "\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "6ApAoMNFfNz6"
      },
      "outputs": [

      ],
      "source": [
        "tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "rF1jrAJsfYW_"
      },
      "outputs": [

      ],
      "source": [
        "tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DK05KXrAAld3"
      },
      "source": [
        "### Gradienten höherer Ordnung\n",
        "\n",
        "Vorgänge innerhalb des `GradientTape` Kontextmanagers werden zur automatischen Unterscheidung aufgezeichnet. Wenn in diesem Zusammenhang Gradienten berechnet werden, wird auch die Gradientenberechnung aufgezeichnet. Als Ergebnis funktioniert die exakt gleiche API auch für Gradienten höherer Ordnung. Zum Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "cPQgthZ7ugRJ"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    y = x * x * x\n",
        "\n",
        "  # Compute the gradient inside the outer `t2` context manager\n",
        "  # which means the gradient computation is differentiable as well.\n",
        "  dy_dx = t1.gradient(y, x)\n",
        "d2y_dx2 = t2.gradient(dy_dx, x)\n",
        "\n",
        "print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0\n",
        "print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0HV-Ah4_76i"
      },
      "source": [
        "Damit erhalten Sie zwar die zweite Ableitung einer *Skalarfunktion* , dieses Muster lässt sich jedoch nicht verallgemeinern, um eine hessische Matrix zu erzeugen, da `GradientTape.gradient` nur den Gradienten eines Skalars berechnet. Um ein Hessisch zu konstruieren, siehe das [hessische Beispiel](#hessian) unter dem [Jacobi-Abschnitt](#jacobians) .\n",
        "\n",
        "\"Verschachtelte Aufrufe von `GradientTape.gradient` \" ist ein gutes Muster, wenn Sie einen Skalar aus einem Gradienten berechnen und der resultierende Skalar dann als Quelle für eine zweite Gradientenberechnung dient, wie im folgenden Beispiel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t7LRlcpVKHv1"
      },
      "source": [
        "#### Beispiel: Regularisierung des Eingabegradienten\n",
        "\n",
        "Viele Modelle sind anfällig für \"konträre Beispiele\". Diese Sammlung von Techniken modifiziert die Eingabe des Modells, um die Ausgabe des Modells zu verwirren. Die [einfachste Implementierung](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) macht einen einzigen Schritt entlang des Gradienten der Ausgabe in Bezug auf die Eingabe; der \"Eingangsgradient\".\n",
        "\n",
        "Eine Technik zur Erhöhung der Robustheit gegenüber gegnerischen Beispielen ist die [Regularisierung](https://arxiv.org/abs/1905.11468) des Eingangsgradienten, die versucht, die Größe des Eingangsgradienten zu minimieren. Wenn der Eingangsgradient klein ist, sollte auch die Änderung des Ausgangs klein sein.\n",
        "\n",
        "Unten ist eine naive Implementierung der Eingabegradienten-Regularisierung. Die Umsetzung ist:\n",
        "\n",
        "1. Berechnen Sie die Steigung des Ausgangs in Bezug auf den Eingang mit einem Innenband.\n",
        "2. Berechnen Sie die Größe dieses Eingabegradienten.\n",
        "3. Berechnen Sie den Gradienten dieser Größe in Bezug auf das Modell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tH3ZFuUfDLrR"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "E6yOFsjEDR9u"
      },
      "outputs": [

      ],
      "source": [
        "with tf.GradientTape() as t2:\n",
        "  # The inner tape only takes the gradient with respect to the input,\n",
        "  # not the variables.\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
        "    t1.watch(x)\n",
        "    y = layer(x)\n",
        "    out = tf.reduce_sum(layer(x)**2)\n",
        "  # 1. Calculate the input gradient.\n",
        "  g1 = t1.gradient(out, x)\n",
        "  # 2. Calculate the magnitude of the input gradient.\n",
        "  g1_mag = tf.norm(g1)\n",
        "\n",
        "# 3. Calculate the gradient of the magnitude with respect to the model.\n",
        "dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "123QMq6PqK_d"
      },
      "outputs": [

      ],
      "source": [
        "[var.shape for var in dg1_mag]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E4xiYigexMtQ"
      },
      "source": [
        "## Jakobiner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4-hVHVIeExkI"
      },
      "source": [
        "Alle vorherigen Beispiele nahmen die Gradienten eines skalaren Ziels in Bezug auf einen oder mehrere Quelltensoren.\n",
        "\n",
        "Die [Jacobi-Matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) repräsentiert die Gradienten einer vektorbewerteten Funktion. Jede Zeile enthält den Gradienten eines der Elemente des Vektors.\n",
        "\n",
        "Mit der `GradientTape.jacobian` können Sie effizient eine Jacobi-Matrix berechnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KzNyIM0QBYIH"
      },
      "source": [
        "Beachten Sie, dass:\n",
        "\n",
        "- Wie `gradient` : Das `sources` kann ein Tensor oder ein Container von Tensoren sein.\n",
        "- Im Gegensatz zu `gradient` : Der `target` Tensor ein einzelner Tensor sein muss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O74K3hlxBC8a"
      },
      "source": [
        "### Skalare Quelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B08OKn1Orkuc"
      },
      "source": [
        "Als erstes Beispiel ist hier der Jacobi-Wert eines Vektor-Targets in Bezug auf eine Skalarquelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "bAFeIE8EuVIq"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "delta = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = tf.nn.sigmoid(x+delta)\n",
        "\n",
        "dy_dx = tape.jacobian(y, delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BgHbUk3zr-WU"
      },
      "source": [
        "Wenn Sie den Jacobi-Wert in Bezug auf einen Skalar nehmen, hat das Ergebnis die Form des **Ziels** und gibt den Gradienten jedes Elements in Bezug auf die Quelle an:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "iZ6awnDzr_BA"
      },
      "outputs": [

      ],
      "source": [
        "print(y.shape)\n",
        "print(dy_dx.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "siNZaklc0_-e"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x.numpy(), y, label='y')\n",
        "plt.plot(x.numpy(), dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DsOMSD_1BGkD"
      },
      "source": [
        "### Tensorquelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g3iXKN7KF-st"
      },
      "source": [
        "Unabhängig davon, ob es sich bei der Eingabe um einen Skalar oder einen Tensor handelt, `GradientTape.jacobian` effizient den Gradienten jedes Elements der Quelle in Bezug auf jedes Element des Ziels bzw. der Ziele.\n",
        "\n",
        "Die Ausgabe dieser Ebene hat beispielsweise die Form `(10, 7)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "39YXItgLxMBk"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = layer(x)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tshNRtfKuVP_"
      },
      "source": [
        "Und die Kernform der Ebene ist `(5, 10)` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CigTWyfPvPuv"
      },
      "outputs": [

      ],
      "source": [
        "layer.kernel.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mN96JRpnAjpx"
      },
      "source": [
        "Die Form des Jacobi-Wertes der Ausgabe in Bezug auf den Kernel sind diese beiden miteinander verketteten Formen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "pRLzTTbvEimH"
      },
      "outputs": [

      ],
      "source": [
        "j = tape.jacobian(y, layer.kernel)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Lrv7miMvTll"
      },
      "source": [
        "Wenn Sie über die Dimensionen des Ziels summieren, verbleibt der Gradient der Summe, der von `GradientTape.gradient` berechnet worden wäre:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "FJjZpYRnDjVa"
      },
      "outputs": [

      ],
      "source": [
        "g = tape.gradient(y, layer.kernel)\n",
        "print('g.shape:', g.shape)\n",
        "\n",
        "j_sum = tf.reduce_sum(j, axis=[0, 1])\n",
        "delta = tf.reduce_max(abs(g - j_sum)).numpy()\n",
        "assert delta < 1e-3\n",
        "print('delta:', delta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZKajuGlk_krs"
      },
      "source": [
        "<a id=\"hessian\"> </a>\n",
        "\n",
        "#### Beispiel: Hessisch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NYcsXeo8TDLi"
      },
      "source": [
        "Während `tf.GradientTape` keine explizite Methode zum Konstruieren einer hessischen Matrix bietet, ist es möglich, eine mit der `GradientTape.jacobian` Methode zu erstellen.\n",
        "\n",
        "Hinweis: Die hessische Matrix enthält `N**2` Parameter. Aus diesem und anderen Gründen ist es für die meisten Modelle nicht praktikabel. Dieses Beispiel dient eher als Demonstration der Verwendung der `GradientTape.jacobian` und ist keine Befürwortung der direkten hessischen Optimierung. Ein hessisches Vektorprodukt kann [mit verschachtelten Bändern effizient berechnet werden](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/hvp_test.py) und ist ein viel effizienterer Ansatz zur Optimierung zweiter Ordnung.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ELGTaell_j81"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
        "\n",
        "with tf.GradientTape() as t2:\n",
        "  with tf.GradientTape() as t1:\n",
        "    x = layer1(x)\n",
        "    x = layer2(x)\n",
        "    loss = tf.reduce_mean(x**2)\n",
        "\n",
        "  g = t1.gradient(loss, layer1.kernel)\n",
        "\n",
        "h = t2.jacobian(g, layer1.kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "FVqQuZj4XGjm"
      },
      "outputs": [

      ],
      "source": [
        "print(f'layer.kernel.shape: {layer1.kernel.shape}')\n",
        "print(f'h.shape: {h.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_M7XElgaiMeP"
      },
      "source": [
        "Um dieses Hessisch für einen Newton-Methodenschritt zu verwenden, würden Sie zuerst seine Achsen zu einer Matrix abflachen und den Gradienten zu einem Vektor abflachen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "6te7N6wVXwXX"
      },
      "outputs": [

      ],
      "source": [
        "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
        "\n",
        "g_vec = tf.reshape(g, [n_params, 1])\n",
        "h_mat = tf.reshape(h, [n_params, n_params])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L9rO8b-0mgOH"
      },
      "source": [
        "Die Hesse-Matrix sollte symmetrisch sein:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8TCHc7Vrf52S"
      },
      "outputs": [

      ],
      "source": [
        "def imshow_zero_center(image, **kwargs):\n",
        "  lim = tf.reduce_max(abs(image))\n",
        "  plt.imshow(image, vmin=-lim, vmax=lim, cmap='seismic', **kwargs)\n",
        "  plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DExOxd7Ok2H0"
      },
      "outputs": [

      ],
      "source": [
        "imshow_zero_center(h_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13fBswmtQes4"
      },
      "source": [
        "Der Aktualisierungsschritt der Newton-Methode wird unten gezeigt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "3DdnbynBdSor"
      },
      "outputs": [

      ],
      "source": [
        "eps = 1e-3\n",
        "eye_eps = tf.eye(h_mat.shape[0])*eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-zPdtyoWeUeV"
      },
      "source": [
        "Hinweis: [Invertieren Sie die Matrix nicht](https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix/) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "k1LYftgmswOO"
      },
      "outputs": [

      ],
      "source": [
        "# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
        "# h_mat = ∇²f(X(k))\n",
        "# g_vec = ∇f(X(k))\n",
        "update = tf.linalg.solve(h_mat + eye_eps, g_vec)\n",
        "\n",
        "# Reshape the update and apply it to the variable.\n",
        "_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pF6qjlHKWxF4"
      },
      "source": [
        "Während dies für eine einzelne `tf.Variable` relativ einfach ist, würde die Anwendung auf ein nicht-triviales Modell eine sorgfältige Verkettung und Aufteilung erfordern, um einen vollständigen Hessian über mehrere Variablen zu erzeugen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PQWM0uN-GO5t"
      },
      "source": [
        "### Batch Jacobian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKtB3rY6EySJ"
      },
      "source": [
        "In einigen Fällen möchten Sie den Jacobi-Wert jedes Zielstapels in Bezug auf einen Quellenstapel nehmen, wobei die Jacobi-Werte für jedes Ziel-Quellen-Paar unabhängig sind.\n",
        "\n",
        "Hier wird beispielsweise der Eingang `x` geformt `(batch, ins)` und der Ausgang `y` geformt `(batch, outs)` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tQMndhIUHMes"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = layer2(y)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ff2spRHEJXBU"
      },
      "source": [
        "Die volle Jacobilinie von `y` in Bezug auf `x` hat die Form `(batch, ins, batch, outs)` , auch wenn Sie nur `(batch, ins, outs)` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "1zSl2A5-HhMH"
      },
      "outputs": [

      ],
      "source": [
        "j = tape.jacobian(y, x)\n",
        "j.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UibJijPLJrpQ"
      },
      "source": [
        "Wenn die Gradienten jedes Elements im Stapel unabhängig sind, dann ist jede `(batch, batch)` Scheibe dieses Tensors eine Diagonalmatrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ZFl9uj3ueVSH"
      },
      "outputs": [

      ],
      "source": [
        "imshow_zero_center(j[:, 0, :, 0])\n",
        "_ = plt.title('A (batch, batch) slice')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "g4ZoRJcJNmy5"
      },
      "outputs": [

      ],
      "source": [
        "def plot_as_patches(j):\n",
        "  # Reorder axes so the diagonals will each form a contiguous patch.\n",
        "  j = tf.transpose(j, [1, 0, 3, 2])\n",
        "  # Pad in between each patch.\n",
        "  lim = tf.reduce_max(abs(j))\n",
        "  j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],\n",
        "             constant_values=-lim)\n",
        "  # Reshape to form a single image.\n",
        "  s = j.shape\n",
        "  j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])\n",
        "  imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])\n",
        "\n",
        "plot_as_patches(j)\n",
        "_ = plt.title('All (batch, batch) slices are diagonal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OXpTBKyeK84z"
      },
      "source": [
        "Um das gewünschte Ergebnis zu erhalten, können Sie über die doppelte `batch` Dimension summieren oder die Diagonalen mit `tf.einsum` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "v65OAjEgLQwl"
      },
      "outputs": [

      ],
      "source": [
        "j_sum = tf.reduce_sum(j, axis=2)\n",
        "print(j_sum.shape)\n",
        "j_select = tf.einsum('bxby->bxy', j)\n",
        "print(j_select.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zT_VfR6lcwxD"
      },
      "source": [
        "Es wäre viel effizienter, die Berechnung von vornherein ohne die zusätzliche Dimension durchzuführen. Die `GradientTape.batch_jacobian` macht genau das."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "YJLIl9WpHqYq"
      },
      "outputs": [

      ],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "jb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "-5t_q5SfHw7T"
      },
      "outputs": [

      ],
      "source": [
        "error = tf.reduce_max(abs(jb - j_sum))\n",
        "assert error < 1e-3\n",
        "print(error.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUeY2ZCiL31I"
      },
      "source": [
        "Achtung: `GradientTape.batch_jacobian` überprüft nur, ob die erste Dimension von Quelle und Ziel übereinstimmen. Es überprüft nicht, ob die Gradienten tatsächlich unabhängig sind. Es liegt am Benutzer, sicherzustellen, dass er `batch_jacobian` nur dort verwendet, wo es sinnvoll ist. Zum Beispiel das Hinzufügen eines `layers.BatchNormalization` zerstört die Unabhängigkeit, da es über die `batch` Dimension hinweg normalisiert:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tnDugVc-L4fj"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.random.normal([7, 5])\n",
        "\n",
        "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
        "bn = tf.keras.layers.BatchNormalization()\n",
        "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
        "\n",
        "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x)\n",
        "  y = layer1(x)\n",
        "  y = bn(y, training=True)\n",
        "  y = layer2(y)\n",
        "\n",
        "j = tape.jacobian(y, x)\n",
        "print(f'j.shape: {j.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "SNyZ1WhJMVLm"
      },
      "outputs": [

      ],
      "source": [
        "plot_as_patches(j)\n",
        "\n",
        "_ = plt.title('These slices are not diagonal')\n",
        "_ = plt.xlabel(\"Don't use `batch_jacobian`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M_x7ih5sarvG"
      },
      "source": [
        "In diesem Fall `batch_jacobian` immer noch und gibt *etwas* mit der erwarteten Form zurück, aber sein Inhalt hat eine unklare Bedeutung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "k8_mICHoasCi"
      },
      "outputs": [

      ],
      "source": [
        "jb = tape.batch_jacobian(y, x)\n",
        "print(f'jb.shape: {jb.shape}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "advanced_autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
